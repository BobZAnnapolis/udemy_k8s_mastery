z21_Updating_Apps_with_Probes_and_Testing
=========================================

Example Steps for DockerCoins
-----------------------------
- adding healthchecks to DockerCoins

Liveness, readiness or both ?
-----------------------------
- to answer that question, we need to see the app run for a while
- do we get temporary, recoverable glitches ?
  - then use READINESS
- or do we get hard lock-ups requiring a restart ?
  - then use liveness
- in the case of DockerCoins, we don't know yet
- let's pick livenessProbe


Do we have HTTP endpoints that we can use ?
-------------------------------------------
- each of the 3 web services (hasher, rng, webui) has a trivial route on /
  - inspect the code and see what they do
  - make sure it's inexpensive
  - if logging - maybe create a new endpoint just for the probes ?
- these routes :
  - don't seem to perform anything complex or expensive
  - don't seem to call other services
- PERFECT !!
  - use these routes


Updating DockerCoins with Probes
--------------------------------
- retrieving DockerCoins manifests
- i've split up the previous dockercoins.yaml into 1-resource-per-file
- this works with the apply command, and is easier for humans to manage
- clone them locally so we can add healthchecks and re-APPLY
  - clone https://github.com/bretfisher/kubercoins
    - cd kubercoins

A simple HTTP liveness probe
----------------------------
containers:
- name: ...
image: ...
livenessProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 30
  periodSeconds: 5

- this will give 30 seconds to the service to start - way more than necessary
- it will run every 5 seconds
- it will use the default timeout - 1 second
- it will use the default failure threshold - 3 failed attempts = dead
- it will use the default success threshold - 1 successful attempt = alive


Adding the liveness probe
-------------------------
- let's add the liveness probe, then deploy DockerCoins
- remember if you don't have DockerCoins running, this will create it
- if you already have DockerCoins running, this will update 'rng'
- edit the 'rng' deployment yaml and add the above liveness code section above
  - vi rng-deployment.yaml
- load the YAML for all the resources of DockerCoins
  - kubectl apply -f .
- when executed like above, will apply ALL *.YAML files
bobzawislak (master *) kubercoins $ kubectl apply -f .
deployment.apps/hasher unchanged
service/hasher unchanged
deployment.apps/redis unchanged
service/redis unchanged
deployment.apps/rng configured
service/rng unchanged
deployment.apps/webui unchanged
service/webui unchanged
deployment.apps/worker configured

- "configured" indicates a change has taken place

Testing Liveness Probe
----------------------
- the 'rng' service needs 100ms to process a request
  - because it is single-threaded and sleeps 0.1s in each request
- the probe timeout is set to 1 second
- if we send more than 10 requests per second per backend. it will break
- let's generate traffic & see what happens
- get the cluster IP address for the rng service
  - kubectl get svc rng
- monitor the rng service
  - in other sessions
    - kubectl get events -w
    - kubectl get pods -w
- jump into shpod, generate traffic
- use 'ab' {Apache Bench tool} to send concurrent requests to rng
  - ab -c 10 -n 1000 http://<clusterIP>/1
  - experiment with higher values of -c and see what happens
- the -c parameter indicates the number of concurrent requests
- the final '/1' is important to generate actual traffic
  - otherwise we would use the ping endpoint, which doesn't sleep 0.1s per request


Making Better Health CHECKS
---------------------------
- above a given threshold, the liveness probe starts failing
  - about 10 concurrent requests per backend should be plenty enough
- when the liveness probe fails 3 times in a row, container is restarted
- during the restart, there is less capacity available
- ... meaning that the other backends are likely to timeout as well
- ... eventually causing all backends to be restarted
- ... and each fresh backend gets restarted too
- this goes on until the load goes down, or we add capacity

- THIS WOULD NOT BE A GOOD HEALTH CHECK IN A REAL application


Better Health CHECKS
====================
- we need to make sure that the healthcheck doesn't trip when performance degrades due to external pressure
- using a readiness check would have fewer effects
  - but it would still be an imperfect solution
- a possible combination
  - readiness check with a short timeout / low failure threshold
  - liveness check with a longer timeout / higher failure threshold

- horizontal pod auto-scaling
  - looks at deployments - uses metrics to determine what to do


EXEC LIVENESS PROBES
--------------------
- healthchecks for redis {1 pod for us}
- a liveness probe is enough
  - it's not useful to remove a backend from rotation when it's the only one
- we could use an exec probe running redis-cli ping


EXEC PROBES & ZOMBIES
---------------------
- when using exec probes, we should make sure that we have a zombie reaper
  - wait, what ? :-)
- when a process terminates, its parent must call wait()/waitpid()
  - this is how the parent process retrieves the child's exit status
- in the meantime, the process is in a 'zombie state'
  - the process state will show as 'Z' in ps, top, ...
- when a process is killed, its children are 'orphaned' and attached to PID 1
- PID 1 has the responsibility of 'reaping' these processes when they terminate
  - ok, but how does that affect us ?


PID 1 in containers
-------------------
- on ordinary systems, PID 1 (/sbin/init) jas logic to reap processes
- in containers, PID 1 is typically our application process
  - eg Apache, JVM, NGINX, REDIS ...
- these do NOT take care of reaping orphans
- if we use exec probes, we need to add a process reaper
- we can add 'tini' to our images ... or ...
  - more widely used & accepted, used in docker-compose
  - have to add into the container image
- share the PID namespace between containers of a pod
  - and have gcr.io/pause take care of reaping
- discussion of this is in "Video 10 - 10 Ways To Shoot Yourself In The Foot w/K8s That Will Surprise You"
  - KUBECON video - DataDog


TINI & REDIS PING in a LIVENESS Probe
-------------------------------------
- add 'tini' to your custom redis image
- change the kubercoins YAML to use your own image
- create a liveness probe in kubercoins YAML
- use 'exec' handler and run 'tini -s -- redis-cli ping'
- example repo here : github.com/BretFisher/redis-tini


CLEANUP AFTER HEALTH CHECKS
---------------------------
- remove our DockerCoin resources for now
  - kubectl delete -f https://k8smastery.com/dockercoins.yaml

- 
