Section 07. Cron Jobs and Resource Creation Options
===================================================

- more RUN command

- creates 'templates' in the background which contain specifications for what to 'run'

- what if we wanted to start a "one-shot" container that DOESN'T get restarted ?
  - 'kubectl run --restart=OnFailure'  or 'kubectl run --restart=Never'
  - these commands would create 'jobs' or 'pods' instead of deployments
- under the hood, 'kubectl run' invokes "generators" to create resource descriptors
- we could also write these resource descriptions ourselves in YAML
  - and create them on a cluster using 'kubectl apply -f fname'
- with 'kubectl run --schedule=...', can create CRON jobs


SCHEDULING PERIODIC BACKGROUND WORK
===================================
- a CRON job is a job that will be executed at specific intervals
- it requires a 'schedule' of when to run, represented as 5 space-separated fields
  - minute  [0,59]
  - hour  [0,23]
  - day of the month  [1,31]
  - month of the year  [1,12]
  - day of the week  [0,6] 0=Sunday
- * means all valid values; "/N" means every "N"
  - eg, '*/3 * * * *' means "every 3 minutes"

- kubectl run --schedule="*/3 * * * *" --restart=OnFailure --image=alpine sleep 10
kubectl run --generator=cronjob/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
cronjob.batch/sleep created
- what happens
  - at the specified schedule, the Cron job will create a job
  - the job will create a pod
  - the job will make sure that the pod completes
    - re-creating another one if it fails, for instance if the node fails


RESOURCE CREATION & RUN CHANGES
===============================
- what about that deprecation warning ?
  - as we can see from the execution, 'kubectl run' can do many things
  - the exact type of resource created is not obvious
- K8s is trying to make 'run' behave more like the 'docker run' command
- to make things more explicit, it is better to use 'kubectl create'
  - 'kubectl create deployment' to create a deployment
  - 'kubectl create job' to create a job
  - 'kubectl create cronjob' to run a job periodically
- eventually, 'kubectl run' will be used only to start 1-shot pods

- 3 ways to 'create' deployments
  - kubectl run
    - easy way
    - versatile
  - kubectl create
    - explicit, but lacks some features
    - can't create a CRONJOB < v1.14
    - can't pass command line arguments to deployments
  - kubectl create -f foo.yaml  or  kubectl apply -f foo.yaml
    - all features available
    - requires writing yaml

- KUBECTL APPLY
  - REQUIRES A YAML file
  - file contains all parms to do everything


STREAMING LOGS of MULTIPLE pods
===============================
- can we stream the logs of all of our pods ?
  - kubectl logs -l run=pingpong --tail 1 -f
    - "-l" - selector
- there is a limit to the number of log streams that can be followed

-$ kubectl scale deployment pingpong --replicas=8
deployment.apps/pingpong scaled
$ kubectl logs -l run=pingpong --tail 1 -f
error: you are attempting to follow 8 log streams, but maximum allowed concurrency is 5,
use --max-log-requests to increase the limit

- why can't we stream the logs of many pods ?
  - 'kubectl' opens 1 connection to the API Server per pod
  - for each pod, the API Server opens 1 extra connection to the corresponding kubelet
  - if there are 1000 pods in our deployment, that's 1000 inbound + 1000 outbound connections on the API Server
  - this could easily put a lot of stress on the API Server
- < v1.14 - was decided to NOT allow multiple connections
- > v1.14 - allowed, but defaults to max=5
  - max overridden using --max-log-requests=N


SHORTCOMINGS OF KUBECTL LOGS
============================
- we don't see which pod sent which log line
- if pods are restarted / replaced, the log stream stops
- if new pods are added, we don't see their LOGS
- to stream the logs of multiple pods, we need to write a selector
- there are external tools to address these SHORTCOMINGS
  - STERN


BETTER CLI LOGS WITH STERN
==========================
- the kubectl logs command has limitations :
  - it cannot stream logs from multiple pods at a time
  - when showing logs from multiple pods, it mixes them all together

- STERN is an open source project
  - allows you to tail multiple pods on K8s and multiple containers within the pod.
  - each result is color-coded for quicker debugging
- the query is a regular expression so the pod name can easily be filtered and you don't need
  to specify the exact id (for instance omitting the deployment id). If a pod is deleted it
  gets removed from tail and if a new pod is added it automatically gets tailed.

- --tail N flag shows the last N lines for each containers
- -t / --timestamps
- --all-namespaces


CLEAN UP PINGPONG & CronJOBS
============================
- remove our deployment and CronJOBS
- 'kubectl delete deployment/pingpong  cronjob/sleep'
  - remember that the actual pingpongs will terminate immediately but will take 30 seconds before
    being removed completely


ASSIGNMENT # 1
==============
- These questions will either ask for you to enter a kubectl command, other shell commands, or ask for a long-form answer. For each one expecting a command, test your answers, and then use that command as the answer to the assignment question.

The goal here is to practice thinking of kubectl commands to solve your own problems, without resorting to looking at answers. It'll help you get used to the CLI if you force yourself to use the --help as much as possible. The long-form questions are to get you thinking about the "why" of things in Kubernetes, which will deepen your understanding.

Questions for this assignment

How many nodes does your cluster have?
(the answer should be the kubectl command you used to get the answer)
We can get a list of nodes with kubectl get nodes.


What kernel version and what container engine is each node running?
(the answer should be the kubectl command you used to get the answer)
kubectl get nodes -o wide will list extra information for each node.


List only the pods in the kube-system namespace.
(the answer should be the kubectl command you used to get the answer)
kubectl get pods --namespace=kube-system


Explain the role of some of these pods.
This depends on how our cluster was set up.
On some clusters, we might see pods named etcd-XXX or kube-apiserver-XXX. these correspond to control plane components.
It's also common to see kubedns-XXX or coredns-XXX.
These implement the DNS service that lets us resolve service names into their service IP's.


If there are few or no pods in kube-system, why could that be?
On some clusters, the control plane is located outside the cluster itself, and not running as containers.
In that case, the control plane won't show up in kube-system, but you can find it on the host with ps aux | grep kube.


Create a deployment using kubectl create that runs the image bretfisher/clock and name it ticktock.
(the answer should be the kubectl command you used)
kubectl create deployment ticktock --image=bretfisher/clock


Increase the number of pods running in that deployment to three.
(the answer should be the kubectl command you used)
kubectl scale deployment ticktock --replicas=3


Use a selector to output only the last line of logs of each container.
(the answer should be the kubectl command you used)
kubectl logs --selector=app=ticktock --tail=1
All the resources created with kubectl create deployment xxx will have the label app=xxx.
If you needed to use a pod selector, you can see them in the resource spec that created them.
In this case that's the ReplicaSet, so kubectl describe replicaset ticktock-xxxxx would help.


BASH history
============

watch kubectl get pods
watch --help
watch kubectl get pods  -n 5
watch --help
watch -n 5 kubectl get pods
kubectl stop -h
kubectl
kubectl run --schedule="*/3 * * * *" --restart=OnFailure --image=alpine sleep 10
kubectl get pods
kubectl get all
kubectl get cronjobs
kubectl get cronjobs
kubectl get pods
kubectl get pods
kubectl logs -l run=pingpong --tail 1 -f
kubectl logs -l run=pingpong --tail 1 -f
kubectl scale deployment pingpong --replicas=8
kubectl get all
kubectl logs -l run=pingpong --tail 1 -f
kubectl logs -l run=pingpong --tail 1 -f --max-log-requests
kubectl logs -l run=pingpong --tail 1 -f --max-log-requests=10
stern
stern pingpong
stern pingpong -n default
stern --tail 1 --timestamps pingpong -n default
stern --tail 1 --timestamps pingpong
stern --tail 1 --timestamps pingpong -n default
stern --tail 1 pingpong -n default
stern --tail 1 --timestamps pingpong -n default
kubectl delete deployment/pingpong cronjob/sleep
kubectl get pods
kubectl get all
kubectl get all
kubectl get all
kubectl get all
history
